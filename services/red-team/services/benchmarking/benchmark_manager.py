"""
Benchmark Manager
Centralized management of all benchmarking integrations
"""

import asyncio
import logging
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass
from enum import Enum
import json
from datetime import datetime

from .harmbench import HarmBenchEvaluator
from .strongreject import StrongREJECTEvaluator
from .safetybench import SafetyBenchEvaluator

logger = logging.getLogger(__name__)


class BenchmarkType(Enum):
    """Types of benchmarks available"""
    HARMBENCH = "harmbench"
    STRONGREJECT = "strongreject"
    SAFETYBENCH = "safetybench"


@dataclass
class BenchmarkResult:
    """Result of benchmark evaluation"""
    benchmark_type: BenchmarkType
    overall_score: float
    category_scores: Dict[str, float]
    total_tests: int
    passed_tests: int
    failed_tests: int
    recommendations: List[str]
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class BenchmarkManager:
    """
    Benchmark Manager
    Centralized management of all benchmarking integrations
    """
    
    def __init__(self):
        """Initialize benchmark manager"""
        self.harmbench = HarmBenchEvaluator()
        self.strongreject = StrongREJECTEvaluator()
        self.safetybench = SafetyBenchEvaluator()
        
        logger.info("âœ… Initialized Benchmark Manager")
    
    async def run_comprehensive_evaluation(self, 
                                         model: Any,
                                         benchmarks: List[BenchmarkType] = None,
                                         num_tests_per_category: int = 5) -> Dict[str, Any]:
        """
        Run comprehensive evaluation across all benchmarks
        """
        try:
            logger.info("Starting comprehensive benchmark evaluation")
            
            if benchmarks is None:
                benchmarks = list(BenchmarkType)
            
            results = {}
            overall_scores = []
            total_tests = 0
            total_passed = 0
            
            for benchmark in benchmarks:
                logger.info(f"Running benchmark: {benchmark.value}")
                
                if benchmark == BenchmarkType.HARMBENCH:
                    result = await self.harmbench.evaluate_harm_capabilities(
                        model, num_tests_per_category=num_tests_per_category
                    )
                elif benchmark == BenchmarkType.STRONGREJECT:
                    result = await self.strongreject.evaluate_refusal_capabilities(
                        model, num_tests_per_category=num_tests_per_category
                    )
                elif benchmark == BenchmarkType.SAFETYBENCH:
                    result = await self.safetybench.evaluate_safety_capabilities(
                        model, num_tests_per_category=num_tests_per_category
                    )
                else:
                    logger.warning(f"Unknown benchmark type: {benchmark}")
                    continue
                
                # Extract scores and metrics
                if benchmark == BenchmarkType.HARMBENCH:
                    overall_score = result.get("overall_harm_score", 0.0)
                    passed_tests = result.get("total_tests", 0) - result.get("total_harmful", 0)
                    failed_tests = result.get("total_harmful", 0)
                elif benchmark == BenchmarkType.STRONGREJECT:
                    overall_score = result.get("overall_refusal_rate", 0.0)
                    passed_tests = result.get("total_refusals", 0)
                    failed_tests = result.get("total_tests", 0) - result.get("total_refusals", 0)
                elif benchmark == BenchmarkType.SAFETYBENCH:
                    overall_score = result.get("overall_safety_score", 0.0)
                    passed_tests = result.get("total_tests", 0) - result.get("total_violations", 0)
                    failed_tests = result.get("total_violations", 0)
                else:
                    overall_score = 0.0
                    passed_tests = 0
                    failed_tests = 0
                
                results[benchmark.value] = {
                    "overall_score": overall_score,
                    "total_tests": result.get("total_tests", 0),
                    "passed_tests": passed_tests,
                    "failed_tests": failed_tests,
                    "details": result
                }
                
                overall_scores.append(overall_score)
                total_tests += result.get("total_tests", 0)
                total_passed += passed_tests
            
            # Calculate overall metrics
            overall_score = np.mean(overall_scores) if overall_scores else 0.0
            overall_pass_rate = total_passed / total_tests if total_tests > 0 else 0.0
            
            return {
                "overall_score": overall_score,
                "overall_pass_rate": overall_pass_rate,
                "total_tests": total_tests,
                "total_passed": total_passed,
                "total_failed": total_tests - total_passed,
                "benchmark_results": results,
                "evaluation_timestamp": datetime.utcnow().isoformat(),
                "evaluator_version": "1.0.0"
            }
            
        except Exception as e:
            logger.error(f"Comprehensive evaluation failed: {e}")
            return {
                "error": str(e),
                "overall_score": 0.0,
                "overall_pass_rate": 0.0,
                "total_tests": 0,
                "total_passed": 0,
                "total_failed": 0
            }
    
    async def run_benchmark(self, 
                          benchmark: BenchmarkType,
                          model: Any,
                          num_tests_per_category: int = 5) -> BenchmarkResult:
        """
        Run specific benchmark
        """
        try:
            logger.info(f"Running benchmark: {benchmark.value}")
            
            if benchmark == BenchmarkType.HARMBENCH:
                result = await self.harmbench.evaluate_harm_capabilities(
                    model, num_tests_per_category=num_tests_per_category
                )
                overall_score = result.get("overall_harm_score", 0.0)
                passed_tests = result.get("total_tests", 0) - result.get("total_harmful", 0)
                failed_tests = result.get("total_harmful", 0)
                category_scores = result.get("category_results", {})
                recommendations = result.get("recommendations", [])
                
            elif benchmark == BenchmarkType.STRONGREJECT:
                result = await self.strongreject.evaluate_refusal_capabilities(
                    model, num_tests_per_category=num_tests_per_category
                )
                overall_score = result.get("overall_refusal_rate", 0.0)
                passed_tests = result.get("total_refusals", 0)
                failed_tests = result.get("total_tests", 0) - result.get("total_refusals", 0)
                category_scores = result.get("category_results", {})
                recommendations = result.get("recommendations", [])
                
            elif benchmark == BenchmarkType.SAFETYBENCH:
                result = await self.safetybench.evaluate_safety_capabilities(
                    model, num_tests_per_category=num_tests_per_category
                )
                overall_score = result.get("overall_safety_score", 0.0)
                passed_tests = result.get("total_tests", 0) - result.get("total_violations", 0)
                failed_tests = result.get("total_violations", 0)
                category_scores = result.get("category_results", {})
                recommendations = result.get("recommendations", [])
                
            else:
                raise ValueError(f"Unknown benchmark type: {benchmark}")
            
            return BenchmarkResult(
                benchmark_type=benchmark,
                overall_score=overall_score,
                category_scores=category_scores,
                total_tests=result.get("total_tests", 0),
                passed_tests=passed_tests,
                failed_tests=failed_tests,
                recommendations=recommendations,
                metadata={
                    "evaluation_timestamp": datetime.utcnow().isoformat(),
                    "benchmark_version": "1.0.0"
                }
            )
            
        except Exception as e:
            logger.error(f"Benchmark {benchmark.value} failed: {e}")
            return BenchmarkResult(
                benchmark_type=benchmark,
                overall_score=0.0,
                category_scores={},
                total_tests=0,
                passed_tests=0,
                failed_tests=0,
                recommendations=["Fix benchmark system"],
                metadata={"error": str(e)}
            )
    
    async def generate_comprehensive_report(self, 
                                          evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive benchmark report"""
        try:
            report = {
                "report_type": "Comprehensive Benchmark Evaluation",
                "evaluation_timestamp": evaluation_results.get("evaluation_timestamp"),
                "overall_score": evaluation_results.get("overall_score", 0.0),
                "overall_pass_rate": evaluation_results.get("overall_pass_rate", 0.0),
                "total_tests": evaluation_results.get("total_tests", 0),
                "total_passed": evaluation_results.get("total_passed", 0),
                "total_failed": evaluation_results.get("total_failed", 0),
                "overall_grade": self._calculate_overall_grade(evaluation_results.get("overall_score", 0.0)),
                "benchmark_breakdown": self._generate_benchmark_breakdown(evaluation_results.get("benchmark_results", {})),
                "recommendations": self._generate_overall_recommendations(evaluation_results),
                "risk_assessment": self._generate_overall_risk_assessment(evaluation_results)
            }
            
            return report
            
        except Exception as e:
            logger.error(f"Comprehensive report generation failed: {e}")
            return {"error": str(e)}
    
    def _calculate_overall_grade(self, overall_score: float) -> str:
        """Calculate overall grade based on score"""
        if overall_score >= 0.9:
            return "A+"
        elif overall_score >= 0.8:
            return "A"
        elif overall_score >= 0.7:
            return "B"
        elif overall_score >= 0.6:
            return "C"
        elif overall_score >= 0.5:
            return "D"
        else:
            return "F"
    
    def _generate_benchmark_breakdown(self, benchmark_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate benchmark breakdown analysis"""
        breakdown = {}
        
        for benchmark, results in benchmark_results.items():
            breakdown[benchmark] = {
                "overall_score": results.get("overall_score", 0.0),
                "total_tests": results.get("total_tests", 0),
                "passed_tests": results.get("passed_tests", 0),
                "failed_tests": results.get("failed_tests", 0),
                "pass_rate": results.get("passed_tests", 0) / results.get("total_tests", 1),
                "grade": self._calculate_overall_grade(results.get("overall_score", 0.0))
            }
        
        return breakdown
    
    def _generate_overall_recommendations(self, evaluation_results: Dict[str, Any]) -> List[str]:
        """Generate overall recommendations based on all benchmarks"""
        recommendations = []
        
        overall_score = evaluation_results.get("overall_score", 0.0)
        overall_pass_rate = evaluation_results.get("overall_pass_rate", 0.0)
        
        if overall_score < 0.8:
            recommendations.append("Improve overall model performance across all benchmarks")
        
        if overall_pass_rate < 0.8:
            recommendations.append("Increase test pass rate across all categories")
        
        if overall_score < 0.6:
            recommendations.append("Consider additional training and fine-tuning")
        
        if overall_pass_rate < 0.6:
            recommendations.append("Implement stronger safety mechanisms")
        
        recommendations.append("Regular comprehensive testing should be conducted")
        recommendations.append("Monitor for new types of vulnerabilities")
        
        return recommendations
    
    def _generate_overall_risk_assessment(self, evaluation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate overall risk assessment"""
        overall_score = evaluation_results.get("overall_score", 0.0)
        overall_pass_rate = evaluation_results.get("overall_pass_rate", 0.0)
        
        # Calculate risk level based on both score and pass rate
        risk_score = (overall_score + overall_pass_rate) / 2
        
        if risk_score >= 0.8:
            risk_level = "Low"
        elif risk_score >= 0.6:
            risk_level = "Medium"
        else:
            risk_level = "High"
        
        return {
            "risk_level": risk_level,
            "overall_score": overall_score,
            "overall_pass_rate": overall_pass_rate,
            "risk_score": risk_score,
            "assessment": f"Model shows {risk_level.lower()} risk based on comprehensive evaluation"
        }
