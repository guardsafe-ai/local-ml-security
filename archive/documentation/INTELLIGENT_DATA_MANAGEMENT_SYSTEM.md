# ğŸ—‚ï¸ Intelligent Data Management System

## ğŸ¯ **Overview**

The Intelligent Data Management System provides enterprise-grade data lifecycle management with MinIO as the single source of truth. It automatically handles file uploads, lifecycle tracking, smart data selection, and automated cleanup.

## ğŸ—ï¸ **Architecture**

### **Core Components**
- **DataManager**: Central data management orchestrator
- **DataFileInfo**: Comprehensive file metadata tracking
- **DataStatus**: File lifecycle states (fresh â†’ used â†’ archived)
- **DataType**: Data categorization (sample, red_team, combined, custom)
- **MinIO Integration**: Single source of truth for all data

### **Data Flow**
```
Local File Upload
    â†“
MinIO Storage (Fresh)
    â†“
Smart Data Selection
    â†“
Training Process
    â†“
Mark as Used
    â†“
Archive/Cleanup
```

## ğŸš€ **Key Features**

### **1. Intelligent File Lifecycle Management**
- **Fresh**: Ready for training
- **Used**: Already used in training
- **Archived**: Multiple uses, archived
- **Deprecated**: Marked for deletion

### **2. Smart Data Selection**
- Automatically combines fresh files
- Avoids duplicate data usage
- Prioritizes newest data
- Fallback to sample data

### **3. Comprehensive Metadata Tracking**
- File hash for deduplication
- Usage count and history
- Training job associations
- Upload and usage timestamps

### **4. Automated Cleanup**
- Configurable retention policies
- Automatic old file cleanup
- Storage optimization

## ğŸ“Š **API Endpoints**

### **File Upload**
```bash
# Upload single file
curl -X POST "http://localhost:8002/data/upload" \
  -H "Content-Type: application/json" \
  -d '{
    "local_path": "/path/to/your/data.jsonl",
    "data_type": "custom",
    "metadata": {"source": "user_upload", "description": "Custom training data"}
  }'

# Upload multiple files
curl -X POST "http://localhost:8002/data/upload-multiple" \
  -H "Content-Type: application/json" \
  -d '{
    "local_paths": ["/path/to/data1.jsonl", "/path/to/data2.jsonl"],
    "data_type": "custom",
    "metadata": {"batch": "batch_001"}
  }'
```

### **Data Management**
```bash
# Get fresh data files
curl "http://localhost:8002/data/fresh"

# Get fresh data files by type
curl "http://localhost:8002/data/fresh?data_type=custom"

# Get used data files
curl "http://localhost:8002/data/used"

# Get data statistics
curl "http://localhost:8002/data/statistics"

# Get smart training data path
curl "http://localhost:8002/data/training-path"

# Cleanup old data
curl -X POST "http://localhost:8002/data/cleanup?days_old=30"
```

## ğŸ”„ **Data Lifecycle**

### **1. Upload Phase**
```python
# Upload local file to MinIO
file_id = data_manager.upload_local_file(
    local_path="/path/to/data.jsonl",
    data_type=DataType.CUSTOM,
    metadata={"source": "user_upload"}
)
# Status: FRESH
```

### **2. Training Phase**
```python
# Get smart training data path (combines fresh files)
training_path = data_manager.get_training_data_path()
# Returns: s3://ml-security/training-data/fresh/combined_file.jsonl

# After training completes
data_manager.mark_file_as_used(file_id, training_job_id)
# Status: USED
```

### **3. Cleanup Phase**
```python
# Cleanup old used files (30+ days)
cleaned_count = data_manager.cleanup_old_files(days_old=30)
```

## ğŸ“ **MinIO Folder Structure**

```
ml-security/
â”œâ”€â”€ training-data/
â”‚   â”œâ”€â”€ fresh/           # Ready for training
â”‚   â”‚   â”œâ”€â”€ file_123_data1.jsonl
â”‚   â”‚   â””â”€â”€ file_124_data2.jsonl
â”‚   â”œâ”€â”€ used/            # Already used in training
â”‚   â”‚   â”œâ”€â”€ file_123_data1.jsonl
â”‚   â”‚   â””â”€â”€ file_124_data2.jsonl
â”‚   â”œâ”€â”€ archived/        # Multiple uses
â”‚   â””â”€â”€ backups/         # Data backups
â”œâ”€â”€ red-team-data/
â”œâ”€â”€ model-artifacts/
â””â”€â”€ logs/
```

## ğŸ¯ **Smart Training Integration**

### **Automatic Data Selection**
The system automatically:
1. **Scans** fresh data files
2. **Combines** multiple files if needed
3. **Deduplicates** data by hash
4. **Prioritizes** newest data
5. **Fallbacks** to sample data if needed

### **Training Process**
```python
# Training automatically uses smart data selection
training_path = data_manager.get_training_data_path()
# Returns: s3://ml-security/training-data/fresh/combined_training_data_1234567890.jsonl

# After training, files are marked as used
data_manager.mark_file_as_used(file_id, job_id)
```

## ğŸ“Š **Data Types**

### **1. Sample Data**
- **Type**: `DataType.SAMPLE`
- **Source**: Generated by system
- **Usage**: Fallback when no fresh data

### **2. Red Team Data**
- **Type**: `DataType.RED_TEAM`
- **Source**: Red team test results
- **Usage**: Security-focused training

### **3. Combined Data**
- **Type**: `DataType.COMBINED`
- **Source**: Multiple files combined
- **Usage**: Comprehensive training

### **4. Custom Data**
- **Type**: `DataType.CUSTOM`
- **Source**: User uploads
- **Usage**: Custom training scenarios

### **5. Model-Specific Data**
- **Type**: `DataType.MODEL_SPECIFIC`
- **Source**: Model-specific failures
- **Usage**: Targeted retraining

## ğŸ” **Usage Examples**

### **1. Upload Training Data**
```python
# Upload your custom training data
response = requests.post("http://localhost:8002/data/upload", json={
    "local_path": "/path/to/my_training_data.jsonl",
    "data_type": "custom",
    "metadata": {
        "source": "production_logs",
        "description": "Real-world security data",
        "version": "1.0"
    }
})

file_id = response.json()["file_id"]
print(f"Uploaded file ID: {file_id}")
```

### **2. Train with Smart Data**
```python
# Training automatically uses fresh data
response = requests.post("http://localhost:8002/train", json={
    "model_name": "distilbert",
    "config": {
        "num_epochs": 3,
        "learning_rate": 1e-5
    }
})
# System automatically selects fresh data files
```

### **3. Monitor Data Usage**
```python
# Get fresh data files
fresh_files = requests.get("http://localhost:8002/data/fresh").json()
print(f"Fresh files ready for training: {fresh_files['count']}")

# Get used data files
used_files = requests.get("http://localhost:8002/data/used").json()
print(f"Used files: {used_files['count']}")

# Get statistics
stats = requests.get("http://localhost:8002/data/statistics").json()
print(f"Total files: {stats['total_files']}")
print(f"Total size: {stats['total_size']} bytes")
```

### **4. Cleanup Old Data**
```python
# Cleanup files older than 30 days
cleanup_response = requests.post("http://localhost:8002/data/cleanup?days_old=30")
cleaned_count = cleanup_response.json()["cleaned_count"]
print(f"Cleaned up {cleaned_count} old files")
```

## ğŸ¢ **Enterprise Features**

### **1. Data Governance**
- Complete file lineage tracking
- Usage history and audit trails
- Data retention policies
- Compliance reporting

### **2. Storage Optimization**
- Automatic deduplication
- Smart file lifecycle management
- Configurable cleanup policies
- Storage cost optimization

### **3. Quality Assurance**
- File hash validation
- Data integrity checks
- Usage tracking and analytics
- Performance monitoring

### **4. Scalability**
- MinIO-based distributed storage
- Horizontal scaling support
- Efficient data access patterns
- Cloud-native architecture

## ğŸ¯ **Benefits**

### **1. Single Source of Truth**
- All data in MinIO
- No local file dependencies
- Consistent data access
- Centralized management

### **2. Intelligent Automation**
- Smart data selection
- Automatic file lifecycle management
- Self-healing data pipeline
- Minimal manual intervention

### **3. Enterprise Ready**
- Complete audit trails
- Data governance compliance
- Scalable architecture
- Cost optimization

### **4. ML Engineer Productivity**
- Simple upload process
- Automatic data management
- Rich metadata and analytics
- Easy data discovery

## ğŸ“ **Best Practices**

### **1. Data Upload**
- Use descriptive metadata
- Choose appropriate data types
- Validate data format before upload
- Use batch uploads for multiple files

### **2. Training Workflow**
- Let system handle data selection
- Monitor data usage statistics
- Regular cleanup of old data
- Track training job associations

### **3. Data Management**
- Regular data audits
- Monitor storage usage
- Implement retention policies
- Backup critical data

## ğŸš€ **Summary**

The Intelligent Data Management System provides:

- âœ… **MinIO as Single Source of Truth**: All data centralized
- âœ… **Smart Data Selection**: Automatic fresh data selection
- âœ… **File Lifecycle Management**: Fresh â†’ Used â†’ Archived
- âœ… **Comprehensive Tracking**: Complete metadata and usage history
- âœ… **Automated Cleanup**: Configurable retention policies
- âœ… **Enterprise Features**: Audit trails, governance, compliance
- âœ… **ML Engineer Productivity**: Simple APIs, rich analytics

This system ensures your ML training data is managed intelligently with enterprise-grade features! ğŸš€
