# 🗂️ Intelligent Data Management System

## 🎯 **Overview**

The Intelligent Data Management System provides enterprise-grade data lifecycle management with MinIO as the single source of truth. It automatically handles file uploads, lifecycle tracking, smart data selection, and automated cleanup.

## 🏗️ **Architecture**

### **Core Components**
- **DataManager**: Central data management orchestrator
- **DataFileInfo**: Comprehensive file metadata tracking
- **DataStatus**: File lifecycle states (fresh → used → archived)
- **DataType**: Data categorization (sample, red_team, combined, custom)
- **MinIO Integration**: Single source of truth for all data

### **Data Flow**
```
Local File Upload
    ↓
MinIO Storage (Fresh)
    ↓
Smart Data Selection
    ↓
Training Process
    ↓
Mark as Used
    ↓
Archive/Cleanup
```

## 🚀 **Key Features**

### **1. Intelligent File Lifecycle Management**
- **Fresh**: Ready for training
- **Used**: Already used in training
- **Archived**: Multiple uses, archived
- **Deprecated**: Marked for deletion

### **2. Smart Data Selection**
- Automatically combines fresh files
- Avoids duplicate data usage
- Prioritizes newest data
- Fallback to sample data

### **3. Comprehensive Metadata Tracking**
- File hash for deduplication
- Usage count and history
- Training job associations
- Upload and usage timestamps

### **4. Automated Cleanup**
- Configurable retention policies
- Automatic old file cleanup
- Storage optimization

## 📊 **API Endpoints**

### **File Upload**
```bash
# Upload single file
curl -X POST "http://localhost:8002/data/upload" \
  -H "Content-Type: application/json" \
  -d '{
    "local_path": "/path/to/your/data.jsonl",
    "data_type": "custom",
    "metadata": {"source": "user_upload", "description": "Custom training data"}
  }'

# Upload multiple files
curl -X POST "http://localhost:8002/data/upload-multiple" \
  -H "Content-Type: application/json" \
  -d '{
    "local_paths": ["/path/to/data1.jsonl", "/path/to/data2.jsonl"],
    "data_type": "custom",
    "metadata": {"batch": "batch_001"}
  }'
```

### **Data Management**
```bash
# Get fresh data files
curl "http://localhost:8002/data/fresh"

# Get fresh data files by type
curl "http://localhost:8002/data/fresh?data_type=custom"

# Get used data files
curl "http://localhost:8002/data/used"

# Get data statistics
curl "http://localhost:8002/data/statistics"

# Get smart training data path
curl "http://localhost:8002/data/training-path"

# Cleanup old data
curl -X POST "http://localhost:8002/data/cleanup?days_old=30"
```

## 🔄 **Data Lifecycle**

### **1. Upload Phase**
```python
# Upload local file to MinIO
file_id = data_manager.upload_local_file(
    local_path="/path/to/data.jsonl",
    data_type=DataType.CUSTOM,
    metadata={"source": "user_upload"}
)
# Status: FRESH
```

### **2. Training Phase**
```python
# Get smart training data path (combines fresh files)
training_path = data_manager.get_training_data_path()
# Returns: s3://ml-security/training-data/fresh/combined_file.jsonl

# After training completes
data_manager.mark_file_as_used(file_id, training_job_id)
# Status: USED
```

### **3. Cleanup Phase**
```python
# Cleanup old used files (30+ days)
cleaned_count = data_manager.cleanup_old_files(days_old=30)
```

## 📁 **MinIO Folder Structure**

```
ml-security/
├── training-data/
│   ├── fresh/           # Ready for training
│   │   ├── file_123_data1.jsonl
│   │   └── file_124_data2.jsonl
│   ├── used/            # Already used in training
│   │   ├── file_123_data1.jsonl
│   │   └── file_124_data2.jsonl
│   ├── archived/        # Multiple uses
│   └── backups/         # Data backups
├── red-team-data/
├── model-artifacts/
└── logs/
```

## 🎯 **Smart Training Integration**

### **Automatic Data Selection**
The system automatically:
1. **Scans** fresh data files
2. **Combines** multiple files if needed
3. **Deduplicates** data by hash
4. **Prioritizes** newest data
5. **Fallbacks** to sample data if needed

### **Training Process**
```python
# Training automatically uses smart data selection
training_path = data_manager.get_training_data_path()
# Returns: s3://ml-security/training-data/fresh/combined_training_data_1234567890.jsonl

# After training, files are marked as used
data_manager.mark_file_as_used(file_id, job_id)
```

## 📊 **Data Types**

### **1. Sample Data**
- **Type**: `DataType.SAMPLE`
- **Source**: Generated by system
- **Usage**: Fallback when no fresh data

### **2. Red Team Data**
- **Type**: `DataType.RED_TEAM`
- **Source**: Red team test results
- **Usage**: Security-focused training

### **3. Combined Data**
- **Type**: `DataType.COMBINED`
- **Source**: Multiple files combined
- **Usage**: Comprehensive training

### **4. Custom Data**
- **Type**: `DataType.CUSTOM`
- **Source**: User uploads
- **Usage**: Custom training scenarios

### **5. Model-Specific Data**
- **Type**: `DataType.MODEL_SPECIFIC`
- **Source**: Model-specific failures
- **Usage**: Targeted retraining

## 🔍 **Usage Examples**

### **1. Upload Training Data**
```python
# Upload your custom training data
response = requests.post("http://localhost:8002/data/upload", json={
    "local_path": "/path/to/my_training_data.jsonl",
    "data_type": "custom",
    "metadata": {
        "source": "production_logs",
        "description": "Real-world security data",
        "version": "1.0"
    }
})

file_id = response.json()["file_id"]
print(f"Uploaded file ID: {file_id}")
```

### **2. Train with Smart Data**
```python
# Training automatically uses fresh data
response = requests.post("http://localhost:8002/train", json={
    "model_name": "distilbert",
    "config": {
        "num_epochs": 3,
        "learning_rate": 1e-5
    }
})
# System automatically selects fresh data files
```

### **3. Monitor Data Usage**
```python
# Get fresh data files
fresh_files = requests.get("http://localhost:8002/data/fresh").json()
print(f"Fresh files ready for training: {fresh_files['count']}")

# Get used data files
used_files = requests.get("http://localhost:8002/data/used").json()
print(f"Used files: {used_files['count']}")

# Get statistics
stats = requests.get("http://localhost:8002/data/statistics").json()
print(f"Total files: {stats['total_files']}")
print(f"Total size: {stats['total_size']} bytes")
```

### **4. Cleanup Old Data**
```python
# Cleanup files older than 30 days
cleanup_response = requests.post("http://localhost:8002/data/cleanup?days_old=30")
cleaned_count = cleanup_response.json()["cleaned_count"]
print(f"Cleaned up {cleaned_count} old files")
```

## 🏢 **Enterprise Features**

### **1. Data Governance**
- Complete file lineage tracking
- Usage history and audit trails
- Data retention policies
- Compliance reporting

### **2. Storage Optimization**
- Automatic deduplication
- Smart file lifecycle management
- Configurable cleanup policies
- Storage cost optimization

### **3. Quality Assurance**
- File hash validation
- Data integrity checks
- Usage tracking and analytics
- Performance monitoring

### **4. Scalability**
- MinIO-based distributed storage
- Horizontal scaling support
- Efficient data access patterns
- Cloud-native architecture

## 🎯 **Benefits**

### **1. Single Source of Truth**
- All data in MinIO
- No local file dependencies
- Consistent data access
- Centralized management

### **2. Intelligent Automation**
- Smart data selection
- Automatic file lifecycle management
- Self-healing data pipeline
- Minimal manual intervention

### **3. Enterprise Ready**
- Complete audit trails
- Data governance compliance
- Scalable architecture
- Cost optimization

### **4. ML Engineer Productivity**
- Simple upload process
- Automatic data management
- Rich metadata and analytics
- Easy data discovery

## 📝 **Best Practices**

### **1. Data Upload**
- Use descriptive metadata
- Choose appropriate data types
- Validate data format before upload
- Use batch uploads for multiple files

### **2. Training Workflow**
- Let system handle data selection
- Monitor data usage statistics
- Regular cleanup of old data
- Track training job associations

### **3. Data Management**
- Regular data audits
- Monitor storage usage
- Implement retention policies
- Backup critical data

## 🚀 **Summary**

The Intelligent Data Management System provides:

- ✅ **MinIO as Single Source of Truth**: All data centralized
- ✅ **Smart Data Selection**: Automatic fresh data selection
- ✅ **File Lifecycle Management**: Fresh → Used → Archived
- ✅ **Comprehensive Tracking**: Complete metadata and usage history
- ✅ **Automated Cleanup**: Configurable retention policies
- ✅ **Enterprise Features**: Audit trails, governance, compliance
- ✅ **ML Engineer Productivity**: Simple APIs, rich analytics

This system ensures your ML training data is managed intelligently with enterprise-grade features! 🚀
